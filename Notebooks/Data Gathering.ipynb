{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a86b04-bb0c-4208-bb1a-acfd7a42c87b",
   "metadata": {},
   "source": [
    "# Amazon Reviews Dataset Handler ðŸ› ï¸\n",
    "\n",
    "A Python utility for downloading, processing, and managing the McAuley-Lab/Amazon-Reviews-2023 dataset.\n",
    "\n",
    "## Features âœ¨\n",
    "- **Cache Management**\n",
    "- **Dataset Compression** (supports gz/bz2/xz)\n",
    "- **Batch Downloading**\n",
    "- **Error Handling**\n",
    "- **Automatic Cleanup**\n",
    "\n",
    "## Data Ingestion Pipeline Functions \n",
    "### Cache Management\n",
    "\n",
    "#### `get_cache_directory(verbose=True) -> Path`\n",
    "**Purpose**: Manages Hugging Face dataset cache locations  \n",
    "**Parameters**:\n",
    "- `verbose` (bool): When `True`, prints cache path and setup instructions  \n",
    "**Returns**:\n",
    "- `Path` object pointing to Hugging Face cache directory\n",
    "\n",
    "---\n",
    "\n",
    "##### `delete_cache_directory() -> None`\n",
    "**Purpose**: Safely clears Hugging Face cache  \n",
    "**Behavior**:\n",
    "- Deletes entire cache directory\n",
    "- Handles missing directories gracefully\n",
    "- Prints status messages\n",
    "\n",
    "---\n",
    "### Compression utility\n",
    "#### `compress_folder(folder, compression_format=\"gz\", level=6) -> Path` \n",
    "**Purpose**: Compresses dataset folders  \n",
    "**Parameters**:\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `folder` | `Path` | Required | Directory to compress |\n",
    "| `compression_format` | `Literal[\"gz\",\"bz2\",\"xz\"]` | \"gz\" | Compression algorithm |\n",
    "| `level` | `int` | 6 | Compression level (1-9) |\n",
    "\n",
    "**Supported Formats**:\n",
    "```python\n",
    "\"gz\"  # Fastest compression\n",
    "\"bz2\" # Balanced speed/size\n",
    "\"xz\"  # Slowest but smallest\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### `process_dataset(dataset_type, category, ...) -> str`\n",
    "**Data Flow**:\n",
    "1. Checks for existing files\n",
    "2. Downloads dataset from Hugging Face\n",
    "3. Saves to disk\n",
    "4. Compresses if requested\n",
    "\n",
    "**Possible Outputs**:\n",
    "```\n",
    "[SKIP] raw_review_Books already exists\n",
    "[DONE] raw_meta_Electronics downloaded and compressed with XZ level 9\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Main Controller Function\n",
    "\n",
    "#### `download_all_amazon_reviews(base_save_path, ...)`\n",
    "**Full Signature**:\n",
    "```python\n",
    "def download_all_amazon_reviews(\n",
    "    base_save_path: Union[str, Path],\n",
    "    categories: Optional[List[str]] = None,\n",
    "    compress: bool = False,\n",
    "    compression_format: Literal[\"gz\",\"bz2\",\"xz\"] = \"gz\",\n",
    "    compression_level: int = 6\n",
    ") -> None\n",
    "```\n",
    "\n",
    "**Parameter Table**:\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `base_save_path` | `str/Path` | Required | Root save directory |\n",
    "| `categories` | `List[str]` | All 34 categories | Subset selection |\n",
    "| `compress` | `bool` | False | Enable compression |\n",
    "| `compression_format` | `str` | \"gz\" | Compression type |\n",
    "| `compression_level` | `int` | 6 | 1-9 compression strength |\n",
    "\n",
    "**Safety Features**:\n",
    "- Validates category names\n",
    "- Prevents path collisions\n",
    "- Automatic cache cleanup\n",
    "- Progress tracking with `yaspin`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fddb7a10-99c6-4cef-aa55-a98a75af5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the stuff below if you want to get rid of HF Symlink warning on Windows\n",
    "# ====================================================================================\n",
    "# import os\n",
    "# os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = \"1\"\n",
    "# ====================================================================================\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "from datasets import load_dataset, config, Dataset, DatasetDict, load_from_disk\n",
    "from yaspin import yaspin\n",
    "from typing import Optional, Union, List, Literal\n",
    "import uuid\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"yaspin.core\")\n",
    "\n",
    "# Type alias for compression formats\n",
    "CompressionFormat = Literal[\"gz\", \"bz2\", \"xz\"]\n",
    "\n",
    "\n",
    "def get_cache_directory(verbose: bool = True) -> Path:\n",
    "    \"\"\"\n",
    "    Returns the current Hugging Face datasets cache directory as a Path object.\n",
    "\n",
    "    Note:\n",
    "    If you want to use a custom cache directory, you must set the\n",
    "    HF_DATASETS_CACHE environment variable *before* importing anything from `datasets`.\n",
    "    For example:\n",
    "\n",
    "        import os\n",
    "        os.environ[\"HF_DATASETS_CACHE\"] = \"C:\\\\your\\\\custom\\\\path\"\n",
    "\n",
    "        from datasets import load_dataset  # Import AFTER setting the env variable\n",
    "    \"\"\"\n",
    "    cache_dir = Path(config.HF_DATASETS_CACHE)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[INFO] Current cache directory: {cache_dir}\")\n",
    "\n",
    "        print(\n",
    "            \"[NOTE] To use a custom cache directory, set HF_DATASETS_CACHE before importing datasets.\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"    import os\\n\"\n",
    "            \"    os.environ['HF_DATASETS_CACHE'] = 'C:\\\\\\\\your\\\\\\\\custom\\\\\\\\path'\\n\"\n",
    "            \"    from datasets import load_dataset\\n\"\n",
    "        )\n",
    "\n",
    "    return cache_dir\n",
    "\n",
    "\n",
    "def delete_cache_directory() -> None:\n",
    "    \"\"\"\n",
    "    Deletes the Hugging Face datasets cache directory using the path from datasets.config.\n",
    "    \"\"\"\n",
    "    cache_path = Path(config.HF_DATASETS_CACHE)\n",
    "    print(f\"[INFO] Deleting Hugging Face cache at: {cache_path}\")\n",
    "\n",
    "    if cache_path.exists():\n",
    "        shutil.rmtree(cache_path, ignore_errors=True)\n",
    "        print(\"[SUCCESS] Cache directory deleted.\")\n",
    "    else:\n",
    "        print(f\"[WARNING] Cache directory does not exist: {cache_path}\")\n",
    "\n",
    "\n",
    "def default_cache_path() -> Path:\n",
    "    \"\"\"\n",
    "    Returns and prints the default Hugging Face datasets cache path.\n",
    "    \"\"\"\n",
    "    default_path = Path.home() / \".cache\" / \"huggingface\" / \"datasets\"\n",
    "    print(f'[INFO] Your default cache path: \"{default_path}\"')\n",
    "    return default_path\n",
    "\n",
    "\n",
    "# list of available categories\n",
    "VALID_CATEGORIES = [\n",
    "    \"All_Beauty\", \"Amazon_Fashion\", \"Appliances\", \"Arts_Crafts_and_Sewing\", \"Automotive\",\n",
    "    \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Books\", \"CDs_and_Vinyl\",\n",
    "    \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewelry\", \"Digital_Music\", \"Electronics\",\n",
    "    \"Gift_Cards\", \"Grocery_and_Gourmet_Food\", \"Handmade_Products\", \"Health_and_Household\",\n",
    "    \"Health_and_Personal_Care\", \"Home_and_Kitchen\", \"Industrial_and_Scientific\", \"Kindle_Store\",\n",
    "    \"Magazine_Subscriptions\", \"Movies_and_TV\", \"Musical_Instruments\", \"Office_Products\",\n",
    "    \"Patio_Lawn_and_Garden\", \"Pet_Supplies\", \"Software\", \"Sports_and_Outdoors\",\n",
    "    \"Subscription_Boxes\", \"Tools_and_Home_Improvement\", \"Toys_and_Games\", \"Video_Games\", \"Unknown\"\n",
    "]\n",
    "\n",
    "\n",
    "def compress_folder(folder: Path, compression_format: CompressionFormat = \"gz\", level: int = 6) -> Path:\n",
    "    \"\"\"\n",
    "    Compress a folder into a tar.gz archive and delete the original folder.\n",
    "\n",
    "    Args:\n",
    "        folder: Path to the folder to compress\n",
    "        compression_format: Compression format to use - \"gz\" (fastest), \"bz2\" (medium), \"xz\" (highest compression)\n",
    "        level: Compression level (1-9, where 1 is fastest and 9 is highest compression)\n",
    "\n",
    "    Returns:\n",
    "        Path to the created archive\n",
    "    \"\"\"\n",
    "    # validate compression level\n",
    "    if not 1 <= level <= 9:\n",
    "        raise ValueError(f\"Compression level must be between 1 and 9, got {level}\")\n",
    "\n",
    "    # set correct file extension based on format\n",
    "    if compression_format == \"gz\":\n",
    "        ext = \".tar.gz\"\n",
    "        mode = f\"w:gz\"\n",
    "    elif compression_format == \"bz2\":\n",
    "        ext = \".tar.bz2\"\n",
    "        mode = f\"w:bz2\"\n",
    "    elif compression_format == \"xz\":\n",
    "        ext = \".tar.xz\"\n",
    "        mode = f\"w:xz\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported compression format: {compression_format}\")\n",
    "\n",
    "    archive_path = folder.with_suffix(ext)\n",
    "\n",
    "    # gzip allows us to set compression level directly\n",
    "    if compression_format == \"gz\":\n",
    "        with tarfile.open(archive_path, mode, compresslevel=level) as tar:\n",
    "            tar.add(folder, arcname=folder.name)\n",
    "    else:\n",
    "        # For bz2 and xz, we need to handle differently\n",
    "        with tarfile.open(archive_path, mode) as tar:\n",
    "            tar.add(folder, arcname=folder.name)\n",
    "\n",
    "        # Display info about compression format\n",
    "        print(f\"[INFO] Using {compression_format.upper()} compression (level {level}) - this may take some time...\")\n",
    "\n",
    "    # Remove the original folder after successful compression\n",
    "    shutil.rmtree(folder)\n",
    "    return archive_path\n",
    "\n",
    "\n",
    "def process_dataset(dataset_type: str, category: str, base_save_path: Path, compress: bool, compression_format: CompressionFormat = \"gz\", compression_level: int = 6) -> str:\n",
    "    \"\"\"\n",
    "    Download and save a specific dataset type for a category.\n",
    "\n",
    "    Args:\n",
    "        dataset_type: Type of dataset (\"review\" or \"meta\")\n",
    "        category: Category name\n",
    "        base_save_path: Base path to save datasets\n",
    "        compress: Whether to compress the dataset after downloading\n",
    "        compression_format: Format to use for compression (\"gz\", \"bz2\", or \"xz\")\n",
    "        compression_level: Compression level (1-9, where 9 is highest compression)\n",
    "    \"\"\"\n",
    "    folder_name = f\"raw_{dataset_type}_{category}\"\n",
    "    dataset_path = base_save_path / folder_name\n",
    "\n",
    "    # check for existing files with any of the possible extensions\n",
    "    compressed_paths = [\n",
    "        dataset_path.with_suffix(\".tar.gz\"),\n",
    "        dataset_path.with_suffix(\".tar.bz2\"),\n",
    "        dataset_path.with_suffix(\".tar.xz\")\n",
    "    ]\n",
    "\n",
    "    # skip if already exists in any format\n",
    "    if dataset_path.exists() or any(path.exists() for path in compressed_paths):\n",
    "        return f\"[SKIP] {folder_name} already exists\"\n",
    "\n",
    "    # download and save\n",
    "    dataset = load_dataset(\n",
    "        \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "        f\"raw_{dataset_type}_{category}\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    dataset_path.mkdir(parents=True, exist_ok=True)\n",
    "    dataset.save_to_disk(str(dataset_path))\n",
    "\n",
    "    # compress if requested\n",
    "    if compress:\n",
    "        compress_folder(dataset_path, compression_format=compression_format, level=compression_level)\n",
    "        return f\"[DONE] {folder_name} downloaded and compressed with {compression_format.upper()} level {compression_level}\"\n",
    "\n",
    "    return f\"[DONE] {folder_name} downloaded\"\n",
    "\n",
    "\n",
    "def download_all_amazon_reviews(base_save_path: Union[str, Path], categories: Optional[List[str]] = None, compress: bool = False, compression_format: CompressionFormat = \"gz\", compression_level: int = 6) -> None:\n",
    "    \"\"\"\n",
    "    Download Amazon review datasets for specified categories.\n",
    "\n",
    "    Args:\n",
    "        base_save_path: Directory to save the datasets\n",
    "        categories: List of categories to download (defaults to all)\n",
    "        compress: Whether to compress each dataset after downloading\n",
    "        compression_format: Format to use for compression (\"gz\", \"bz2\", or \"xz\")\n",
    "            - \"gz\": Fastest compression, moderate file size (default)\n",
    "            - \"bz2\": Medium compression speed, smaller file size\n",
    "            - \"xz\": Slowest compression, smallest file size\n",
    "        compression_level: Compression level (1-9)\n",
    "            - 1: Fastest compression, largest file size\n",
    "            - 9: Slowest compression, smallest file size\n",
    "            - Default is 6 for a balance of speed and size\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If invalid categories are specified or if paths overlap\n",
    "    \"\"\"\n",
    "    # validate categories\n",
    "    if categories is None:\n",
    "        categories = VALID_CATEGORIES\n",
    "    else:\n",
    "        invalid = set(categories) - set(VALID_CATEGORIES)\n",
    "        if invalid:\n",
    "            raise ValueError(f\"Invalid categories: {invalid}\")\n",
    "\n",
    "    # validate compression options\n",
    "    if not 1 <= compression_level <= 9:\n",
    "        raise ValueError(f\"Compression level must be between 1 and 9, got {compression_level}\")\n",
    "\n",
    "    if compression_format not in [\"gz\", \"bz2\", \"xz\"]:\n",
    "        raise ValueError(f\"Unsupported compression format: {compression_format}. Use 'gz', 'bz2', or 'xz'\")\n",
    "\n",
    "    hf_datasets_cache = get_cache_directory(verbose=False)\n",
    "    base_save_path = Path(base_save_path).resolve()\n",
    "    cache_path = Path(hf_datasets_cache).expanduser().resolve()\n",
    "\n",
    "    # check for path overlap\n",
    "    if (base_save_path == cache_path or\n",
    "            base_save_path in cache_path.parents or\n",
    "            cache_path in base_save_path.parents):\n",
    "        raise ValueError(\"âŒ base_save_path and HF_DATASETS_CACHE must be separate and non-overlapping.\")\n",
    "\n",
    "    # create base dir if it doesn't exist\n",
    "    base_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # process each category\n",
    "    successful = []\n",
    "    failed = []\n",
    "\n",
    "    # print compression info if compressing\n",
    "    if compress:\n",
    "        print(f\"[INFO] Using {compression_format.upper()} compression at level {compression_level}\")\n",
    "        print(\n",
    "            f\"[INFO] Compression speed: {'Fast' if compression_level < 4 else 'Medium' if compression_level < 7 else 'Slow'}\")\n",
    "        print(\n",
    "            f\"[INFO] Compression ratio: {'Low' if compression_level < 4 else 'Medium' if compression_level < 7 else 'High'}\")\n",
    "\n",
    "    for category in categories:\n",
    "        with yaspin(text=f\"Processing {category}\") as spinner:\n",
    "            try:\n",
    "                # review dataset\n",
    "                review_result = process_dataset(\n",
    "                    \"review\",\n",
    "                    category,\n",
    "                    base_save_path,\n",
    "                    compress,\n",
    "                    compression_format,\n",
    "                    compression_level\n",
    "                )\n",
    "                spinner.write(review_result)\n",
    "\n",
    "                # meta dataset\n",
    "                meta_result = process_dataset(\n",
    "                    \"meta\",\n",
    "                    category,\n",
    "                    base_save_path,\n",
    "                    compress,\n",
    "                    compression_format,\n",
    "                    compression_level\n",
    "                )\n",
    "                spinner.write(meta_result)\n",
    "\n",
    "                spinner.ok(\"âœ…\")\n",
    "                successful.append(category)\n",
    "            except Exception as e:\n",
    "                spinner.fail(\"ðŸ’¥\")\n",
    "                spinner.write(f\"Failed to process category '{category}': {str(e)}\")\n",
    "                failed.append((category, str(e)))\n",
    "            finally:\n",
    "                # clean up cache after each category\n",
    "                if cache_path.exists():\n",
    "                    shutil.rmtree(cache_path, ignore_errors=True)\n",
    "\n",
    "    # print summary\n",
    "    print(f\"\\nðŸŽ‰ Download summary:\")\n",
    "    print(f\"  - Successfully processed: {len(successful)}/{len(categories)} categories\")\n",
    "    if failed:\n",
    "        print(f\"  - Failed: {len(failed)}/{len(categories)} categories\")\n",
    "        for category, error in failed:\n",
    "            print(f\"    - {category}: {error}\")\n",
    "\n",
    "\n",
    "def load_compressed_dataset(compressed_path: Union[str, Path], extract_dir: Optional[Union[str, Path]] = None, cleanup_after_load: bool = True) -> Union[Dataset, DatasetDict]:\n",
    "    \"\"\"\n",
    "    Load a dataset from a compressed archive (tar.gz, tar.bz2, or tar.xz).\n",
    "\n",
    "    Args:\n",
    "        compressed_path: Path to the compressed dataset file\n",
    "        extract_dir: Directory to extract files to (defaults to a temporary directory)\n",
    "        cleanup_after_load: Whether to delete the extracted files after loading\n",
    "                           (only applies to auto-generated temp directories)\n",
    "\n",
    "    Returns:\n",
    "        The loaded dataset (Dataset or DatasetDict)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the file doesn't exist or isn't a supported compressed file\n",
    "    \"\"\"\n",
    "    compressed_path = Path(compressed_path)\n",
    "\n",
    "    if not compressed_path.exists():\n",
    "        raise ValueError(f\"File not found: {compressed_path}\")\n",
    "\n",
    "    # check file extension\n",
    "    valid_extensions = [\".tar.gz\", \".tar.bz2\", \".tar.xz\"]\n",
    "    is_valid = False\n",
    "\n",
    "    for ext in valid_extensions:\n",
    "        if compressed_path.name.endswith(ext):\n",
    "            is_valid = True\n",
    "            break\n",
    "\n",
    "    if not is_valid:\n",
    "        raise ValueError(f\"Expected a compressed tar file (.tar.gz, .tar.bz2, or .tar.xz), got: {compressed_path}\")\n",
    "\n",
    "    # get the expected directory name (remove the extension)\n",
    "    dir_name = compressed_path.name\n",
    "    for ext in valid_extensions:\n",
    "        if dir_name.endswith(ext):\n",
    "            dir_name = dir_name[:-len(ext)]\n",
    "            break\n",
    "\n",
    "    # create extraction directory\n",
    "    is_temp_dir = extract_dir is None\n",
    "    if is_temp_dir:\n",
    "        extract_dir = compressed_path.parent / f\"temp_{uuid.uuid4().hex}\"\n",
    "    else:\n",
    "        extract_dir = Path(extract_dir)\n",
    "\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # extract archive\n",
    "        print(f\"Extracting {compressed_path} to {extract_dir}...\")\n",
    "        with tarfile.open(compressed_path, \"r:*\") as tar:\n",
    "            tar.extractall(path=extract_dir)\n",
    "\n",
    "        # dataset should be in a subdirectory matching the original directory name\n",
    "        dataset_dir = extract_dir / dir_name\n",
    "\n",
    "        if not dataset_dir.exists():\n",
    "            # try to find any directory\n",
    "            extracted_folders = [f for f in extract_dir.iterdir() if f.is_dir()]\n",
    "            if not extracted_folders:\n",
    "                raise ValueError(f\"No folders found in extracted archive: {compressed_path}\")\n",
    "            dataset_dir = extracted_folders[0]\n",
    "            print(f\"Using extracted directory: {dataset_dir}\")\n",
    "\n",
    "        # load dataset\n",
    "        print(f\"Loading dataset from {dataset_dir}...\")\n",
    "        dataset = load_from_disk(str(dataset_dir))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    finally:\n",
    "        # clean up only if it's a temporary directory we created AND cleanup is requested\n",
    "        if cleanup_after_load and is_temp_dir and extract_dir.exists():\n",
    "            print(f\"Cleaning up temporary directory: {extract_dir}\")\n",
    "            shutil.rmtree(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdc3206d-6aee-4f9d-9122-d6c1586280b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] raw_review_All_Beauty already exists\n",
      "[SKIP] raw_meta_All_Beauty already exists\n",
      "âœ… Processing All_Beauty\n",
      "[SKIP] raw_review_Amazon_Fashion already exists\n",
      "[SKIP] raw_meta_Amazon_Fashion already exists\n",
      "âœ… Processing Amazon_Fashion\n",
      "[SKIP] raw_review_Appliances already exists\n",
      "[SKIP] raw_meta_Appliances already exists\n",
      "âœ… Processing Appliances\n",
      "[SKIP] raw_review_Arts_Crafts_and_Sewing already exists\n",
      "[SKIP] raw_meta_Arts_Crafts_and_Sewing already exists\n",
      "âœ… Processing Arts_Crafts_and_Sewing\n",
      "[SKIP] raw_review_Automotive already exists\n",
      "[SKIP] raw_meta_Automotive already exists\n",
      "âœ… Processing Automotive\n",
      "[SKIP] raw_review_Baby_Products already exists\n",
      "[SKIP] raw_meta_Baby_Products already exists\n",
      "âœ… Processing Baby_Products\n",
      "[SKIP] raw_review_Beauty_and_Personal_Care already exists\n",
      "[SKIP] raw_meta_Beauty_and_Personal_Care already exists\n",
      "âœ… Processing Beauty_and_Personal_Care\n",
      "â ‹ Processing Books "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d80ae608c445c09023cefe6be8f3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Books.jsonl:  53%|#####3    | 10.7G/20.1G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â § Processing Books "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081654657d0f4ed68b015ffb01b04f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating full split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¼ Processing Books "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9505a1677a34b18a987d4011c314e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¼ Processing Books "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbff66e384f484d915b0ee307d3669d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/33 shards):   0%|          | 0/29475453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] raw_review_Books downloaded\n",
      "â ¹ Processing Books "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547d1ecdb1b54f87898346f338794b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "meta_Books.jsonl:   0%|          | 0.00/14.7G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ™ Processing Books "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5122bfdc4e74d66b64a80eed26347f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating full split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ´ Processing Books "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226be67a218c4552b8ffcf1b8c41e5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¸ Processing Books "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1dc054f44e46fd82ff96ccd805d7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/28 shards):   0%|          | 0/4448181 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] raw_meta_Books downloaded\n",
      "âœ… Processing Books\n",
      "[SKIP] raw_review_CDs_and_Vinyl already exists\n",
      "[SKIP] raw_meta_CDs_and_Vinyl already exists\n",
      "âœ… Processing CDs_and_Vinyl\n",
      "[SKIP] raw_review_Cell_Phones_and_Accessories already exists\n",
      "[SKIP] raw_meta_Cell_Phones_and_Accessories already exists\n",
      "âœ… Processing Cell_Phones_and_Accessories\n",
      "[SKIP] raw_review_Clothing_Shoes_and_Jewelry already exists\n",
      "[SKIP] raw_meta_Clothing_Shoes_and_Jewelry already exists\n",
      "âœ… Processing Clothing_Shoes_and_Jewelry\n",
      "[SKIP] raw_review_Digital_Music already exists\n",
      "[SKIP] raw_meta_Digital_Music already exists\n",
      "âœ… Processing Digital_Music\n",
      "[SKIP] raw_review_Electronics already exists\n",
      "[SKIP] raw_meta_Electronics already exists\n",
      "âœ… Processing Electronics\n",
      "[SKIP] raw_review_Gift_Cards already exists\n",
      "[SKIP] raw_meta_Gift_Cards already exists\n",
      "âœ… Processing Gift_Cards\n",
      "[SKIP] raw_review_Grocery_and_Gourmet_Food already exists\n",
      "[SKIP] raw_meta_Grocery_and_Gourmet_Food already exists\n",
      "âœ… Processing Grocery_and_Gourmet_Food\n",
      "[SKIP] raw_review_Handmade_Products already exists\n",
      "[SKIP] raw_meta_Handmade_Products already exists\n",
      "âœ… Processing Handmade_Products\n",
      "[SKIP] raw_review_Health_and_Household already exists\n",
      "[SKIP] raw_meta_Health_and_Household already exists\n",
      "âœ… Processing Health_and_Household\n",
      "[SKIP] raw_review_Health_and_Personal_Care already exists\n",
      "[SKIP] raw_meta_Health_and_Personal_Care already exists\n",
      "âœ… Processing Health_and_Personal_Care\n",
      "[SKIP] raw_review_Home_and_Kitchen already exists\n",
      "â § Processing Home_and_Kitchen "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd053abf3547abb9fa3ea636e72735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "meta_Home_and_Kitchen.jsonl:   0%|          | 0.00/11.8G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ™ Processing Home_and_Kitchen "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080bf13f49a3475c9ec22eecc990cb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating full split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ™ Processing Home_and_Kitchen "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca2f5d7f6a744ce96e8126fbf5a60e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¼ Processing Home_and_Kitchen "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8cbb210a4f420f99a370a77742ac17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/21 shards):   0%|          | 0/3735584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] raw_meta_Home_and_Kitchen downloaded\n",
      "âœ… Processing Home_and_Kitchen\n",
      "[SKIP] raw_review_Industrial_and_Scientific already exists\n",
      "[SKIP] raw_meta_Industrial_and_Scientific already exists\n",
      "âœ… Processing Industrial_and_Scientific\n",
      "[SKIP] raw_review_Kindle_Store already exists\n",
      "[SKIP] raw_meta_Kindle_Store already exists\n",
      "âœ… Processing Kindle_Store\n",
      "[SKIP] raw_review_Magazine_Subscriptions already exists\n",
      "[SKIP] raw_meta_Magazine_Subscriptions already exists\n",
      "âœ… Processing Magazine_Subscriptions\n",
      "[SKIP] raw_review_Movies_and_TV already exists\n",
      "[SKIP] raw_meta_Movies_and_TV already exists\n",
      "âœ… Processing Movies_and_TV\n",
      "[SKIP] raw_review_Musical_Instruments already exists\n",
      "[SKIP] raw_meta_Musical_Instruments already exists\n",
      "âœ… Processing Musical_Instruments\n",
      "[SKIP] raw_review_Office_Products already exists\n",
      "[SKIP] raw_meta_Office_Products already exists\n",
      "âœ… Processing Office_Products\n",
      "[SKIP] raw_review_Patio_Lawn_and_Garden already exists\n",
      "[SKIP] raw_meta_Patio_Lawn_and_Garden already exists\n",
      "âœ… Processing Patio_Lawn_and_Garden\n",
      "[SKIP] raw_review_Pet_Supplies already exists\n",
      "[SKIP] raw_meta_Pet_Supplies already exists\n",
      "âœ… Processing Pet_Supplies\n",
      "[SKIP] raw_review_Software already exists\n",
      "[SKIP] raw_meta_Software already exists\n",
      "âœ… Processing Software\n",
      "[SKIP] raw_review_Sports_and_Outdoors already exists\n",
      "[SKIP] raw_meta_Sports_and_Outdoors already exists\n",
      "âœ… Processing Sports_and_Outdoors\n",
      "[SKIP] raw_review_Subscription_Boxes already exists\n",
      "[SKIP] raw_meta_Subscription_Boxes already exists\n",
      "âœ… Processing Subscription_Boxes\n",
      "[SKIP] raw_review_Tools_and_Home_Improvement already exists\n",
      "[SKIP] raw_meta_Tools_and_Home_Improvement already exists\n",
      "âœ… Processing Tools_and_Home_Improvement\n",
      "[SKIP] raw_review_Toys_and_Games already exists\n",
      "[SKIP] raw_meta_Toys_and_Games already exists\n",
      "âœ… Processing Toys_and_Games\n",
      "[SKIP] raw_review_Video_Games already exists\n",
      "[SKIP] raw_meta_Video_Games already exists\n",
      "âœ… Processing Video_Games\n",
      "[SKIP] raw_review_Unknown already exists\n",
      "â ™ Processing Unknown "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1651bc713de44f1d84de5acad3dfaf23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "meta_Unknown.jsonl:   0%|          | 0.00/675M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¸ Processing Unknown "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1028be8eeb8a41599bd1591b3285d5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating full split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ¦ Processing Unknown "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f130642cc8f34240a1effd674eb53128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/390006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] raw_meta_Unknown downloaded\n",
      "âœ… Processing Unknown \n",
      "\n",
      "ðŸŽ‰ Download summary:\n",
      "  - Successfully processed: 34/34 categories\n"
     ]
    }
   ],
   "source": [
    "download_all_amazon_reviews(base_save_path=\"C:\\\\Users\\\\anees\\\\Desktop\\\\Datasets\", compress=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
