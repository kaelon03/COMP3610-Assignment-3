{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d81f94e-567f-4446-9404-2bbb03da41d4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f3dbee7-6aaa-4c19-b3b5-062b5f82583f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\anees\\appdata\\roaming\\python\\python312\\site-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\anees\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\anees\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\anees\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: duckdb in c:\\users\\anees\\appdata\\roaming\\python\\python312\\site-packages (1.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarrow in c:\\programdata\\anaconda3\\lib\\site-packages (16.1.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyarrow) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install duckdb\n",
    "!pip install pyarrow\n",
    "\n",
    "from datasets import Dataset\n",
    "import duckdb\n",
    "import pyarrow as pa\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8e60f-4bd1-4ff2-a4e5-602b12db4166",
   "metadata": {},
   "source": [
    "# Merging Reviews and Metadata for product Categories\n",
    "\n",
    "This script processes product reviews and meta data merges them by category and saves the result as a .parquet file by using DuckDB\n",
    "\n",
    "## Overview\n",
    "The following steps are performed:\n",
    "1. **Creates DuckDB tables** for storing reviews and meatadata\n",
    "2. **Streams data** from all arrow files into two DuckDB tables (Reviews and Metadata)\n",
    "3. **Joins** the two tables on the category\n",
    "4. **Exports the data** to a Parquet file with Zstd compession\n",
    "\n",
    "## Functions\n",
    "\n",
    "``` python\n",
    "def create_tables()\n",
    "``` \n",
    "1. Drops any prexisting review_stream and metastream tables\n",
    "2. Initializes two DuckDB tables:\n",
    "    \n",
    "     review_stream(rating, title, text, images, asin, parent_asin, user_id, timestamp, helpful_vote, verified_purcahse)\n",
    "     \n",
    "     meta_stream(main_category, title, average_rating, rating_number, features, description, price, images, videos, store, categories, details, parent_asin, bought_together,subtitle, author)\n",
    "\n",
    "```python\n",
    "def stream_to_duckdb(table_name, folder):\n",
    "```\n",
    "1. Finds all arrow files in folder\n",
    "2. Read file into hugging face's Dataset to process in batches\n",
    "3. Inserts batch into database through the temporary table (temp_batch)  \n",
    "\n",
    "```python\n",
    "def is_parquet_valid(path):\n",
    "```\n",
    "1. Attempts to read parquet file using DuckDB\n",
    "2. Returns true if it is and false otherwise\n",
    "\n",
    "```python\n",
    "def merge_category(category):\n",
    "```\n",
    "1. Skips categories that already exists\n",
    "2. Loads raw review and metadata for the given category using create_tables and stream_to_db\n",
    "3. joins the review_stream and meta_stream database on parent_asin\n",
    "4. Exports merged data to Parquet file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "984ff5d6-3644-4ad7-865f-2bf1439168f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables():\n",
    "    duckdb.sql(\"DROP TABLE IF EXISTS review_stream\")\n",
    "    duckdb.sql(\"DROP TABLE IF EXISTS meta_stream\")\n",
    "    \n",
    "    duckdb.sql(\"\"\"\n",
    "    CREATE TABLE review_stream (\n",
    "        rating DOUBLE,\n",
    "        title TEXT,\n",
    "        text TEXT,\n",
    "        images TEXT,\n",
    "        asin TEXT,\n",
    "        parent_asin TEXT,\n",
    "        user_id TEXT,\n",
    "        timestamp BIGINT,\n",
    "        helpful_vote BIGINT,\n",
    "        verified_purchase BOOLEAN\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    duckdb.sql(\"\"\"\n",
    "    CREATE TABLE meta_stream (\n",
    "        main_category TEXT,\n",
    "        title TEXT,\n",
    "        average_rating DOUBLE,\n",
    "        rating_number BIGINT,\n",
    "        features TEXT,\n",
    "        description TEXT,\n",
    "        price TEXT,\n",
    "        images TEXT,\n",
    "        videos TEXT,\n",
    "        store TEXT,\n",
    "        categories TEXT,\n",
    "        details TEXT,\n",
    "        parent_asin TEXT,\n",
    "        bought_together TEXT,\n",
    "        subtitle TEXT,\n",
    "        author TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "def stream_to_duckdb(table_name, folder):\n",
    "    files = sorted(glob.glob(os.path.join(folder, \"*.arrow\")))\n",
    "    for path in files:\n",
    "        dataset = Dataset.from_file(path)\n",
    "        reader = dataset.data.to_reader()\n",
    "        for batch in reader:\n",
    "            table = pa.Table.from_batches([batch])\n",
    "            duckdb.register(\"tmp_batch\", table)\n",
    "            duckdb.sql(f\"INSERT INTO {table_name} SELECT * FROM tmp_batch\")\n",
    "            duckdb.unregister(\"tmp_batch\")\n",
    "\n",
    "def is_parquet_valid(path):\n",
    "    try:\n",
    "        duckdb.sql(f\"SELECT COUNT(*) FROM '{path}'\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def merge_category(category):\n",
    "    output_path = os.path.join(base_path, f\"joined_{category}.parquet\")\n",
    "\n",
    "    if os.path.exists(output_path) and is_parquet_valid(output_path):\n",
    "        print(f\"Skipping {category}, already processed.\\n\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing category: {category}\")\n",
    "    \n",
    "    review_folder = os.path.join(base_path, f\"raw_review_{category}\", \"full\")\n",
    "    meta_folder   = os.path.join(base_path, f\"raw_meta_{category}\", \"full\")\n",
    "\n",
    "    create_tables()\n",
    "    stream_to_duckdb(\"review_stream\", review_folder)\n",
    "    stream_to_duckdb(\"meta_stream\", meta_folder)\n",
    "\n",
    "    duckdb.sql(f\"\"\"\n",
    "        COPY (\n",
    "            SELECT \n",
    "                '{category}' AS category,\n",
    "                r.rating,\n",
    "                r.title,\n",
    "                r.text,\n",
    "                r.user_id,\n",
    "                r.asin,\n",
    "                r.parent_asin,\n",
    "                r.timestamp,\n",
    "                r.verified_purchase,\n",
    "                r.helpful_vote,\n",
    "                COALESCE(m.details ->> 'brand', 'Unknown') AS brand,\n",
    "                COALESCE(m.main_category, 'Unknown') AS main_category,\n",
    "                COALESCE(m.store, 'Unknown') AS store,\n",
    "                COALESCE(m.price, 'Unknown') AS price\n",
    "            FROM review_stream r\n",
    "            LEFT JOIN meta_stream m ON r.{join_key} = m.{join_key}\n",
    "        ) TO '{output_path}' (FORMAT PARQUET, COMPRESSION 'zstd');\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"Exported: {output_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d8f3e-a6f5-4c74-9c60-9f82d0169ced",
   "metadata": {},
   "source": [
    "# Process Data\n",
    "This code merges the raw reviews and metadata for all categories in the directory\n",
    "\n",
    "## Variables\n",
    "* **base_path**: Root directory where datasets are stored\n",
    "* **join_key**:  The column used to join review and metadata tables (parent_asin)\n",
    "* **review_folders**: A list of paths to folders matching the pattern raw_review_* .\n",
    "* **categories**: A filtered list of valid category names derived from the folder names.\n",
    "\n",
    "## Function\n",
    "1. Identifies Categories by scanning path for raw reviews\n",
    "2. Ensures raw_review_{category}/full and raw_meta_{category}/full both exist\n",
    "3. If they do the category is added\n",
    "4. If no categories are found an error message is printed\n",
    "5. Otherwise merge_category is used to get the merged parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1eecad6-dbf3-45f8-b5e5-08c0a631f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\anees\\Desktop\\datasets\"\n",
    "join_key = \"parent_asin\"\n",
    "\n",
    "review_folders = glob.glob(os.path.join(base_path, \"raw_review_*\"))\n",
    "categories = []\n",
    "\n",
    "for path in review_folders:\n",
    "    cat = os.path.basename(path).replace(\"raw_review_\", \"\")\n",
    "    review_dir = os.path.join(base_path, f\"raw_review_{cat}\", \"full\")\n",
    "    meta_dir = os.path.join(base_path, f\"raw_meta_{cat}\", \"full\")\n",
    "    if os.path.exists(meta_dir) and os.path.exists(review_dir):\n",
    "        categories.append(cat)\n",
    "if categories == :\n",
    "    print(\"No files found\")\n",
    "else:\n",
    "    for category in categories:\n",
    "        merge_category(category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
